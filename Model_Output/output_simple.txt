'''Hyperparameters'''
batch_size = 32 # how many independent sequences will we process in parallel?
block_size = 8 #maximum number of tokens to be considered as "input" for predictions
n_embed = 384 #dimension of vector after embedding
n_blocks = 6 #number of sequential attention blocks
num_heads = 6

max_iters = 500
eval_interval = 50
learning_rate = 3e-4
device = 'cuda' if torch.cuda.is_available() else 'cpu'
eval_iters = 10
dropout = 0.2

23.073089 M parameters
step 0: train loss 4.2858, val loss 4.2822
step 50: train loss 2.8966, val loss 2.8577
step 100: train loss 2.5107, val loss 2.5362
step 150: train loss 2.4395, val loss 2.4498
step 200: train loss 2.3571, val loss 2.3769
step 250: train loss 2.3198, val loss 2.3271
step 300: train loss 2.3040, val loss 2.2922
step 350: train loss 2.2539, val loss 2.2496
step 400: train loss 2.2060, val loss 2.2751
step 450: train loss 2.1828, val loss 2.1991
step 499: train loss 2.1561, val loss 2.1613

To inke, is you dommaen,
Fior Bet my sepaeinf:
Whengeed of of Thent'pas tice tayints anot that theve, suntor eare to camevesse denden, tealnpes!
As dave of hish and whus me mos.

For Bome.

Aple,
DUd perdoous hear Ewaplice his pereakty's, Lece a Rovesyestit'-ce.
Dh wor'RY'ld of anit of porcayWe tumed, thand kas beeans, swe aurgaion anom your haded?

But ond to itoll on deelon arand--it your wact pareae as with thy cearverl in;
Hust ya us soverst;' se!
Cet?

Whon dofe ton dwith stored,

Bor tove