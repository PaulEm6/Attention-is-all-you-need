Objective: How does GPT work? Really. (Extra Credit, Resume)

Explanation of Theory: Attention is all you need

Simple Implementation based on Tutorial of Andrej:
        Decoder only Transformer (save model_simple and complex):
                - multiples blocks of single head attention
                - separated embed, transformer and output block
                - test results with 5000 iterations (if decent then continue)

        TO DO:
                - implement parralel multi head attention block, applies multiple heads of attention in parralel
                concatenantes the results

        Compare different tokenization methods (sub word tiktoken and sentence piece vs character tokenization)

        Compare optimizer used (impact on training time)

Qualatative and Quantative Analysis of output based on different datasets used (Original code):
    - OneDrive (regex)
    - Game of Thrones (Chapter)
    - Lyrics of on artist (web scrap)
    - Little Shakespeare (original)

Extension: 
        - Self attention       
        - Sentiment analysis
        - Multi block multi head attention