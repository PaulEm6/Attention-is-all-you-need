# Attention is all you need
 Creating a decoder only transformer with self attention blocks based on "Attention is all you need" paper and Andrej Kaparthy Youtube tutorial
